 <div class="user-details">
  <h1 align="center">What We Do</h1>
  <p> We develop advanced <strong>driver assistance</strong> and
  <strong>autonomous driving</strong> systems, based on <strong>computer 
  vision</strong> and <strong>modern machine learning</strong> 
  techniques. We target the sweet spot between the two extremes:
  </p>
  <p>
  <ul>
  <li>"Old-school" driver assistance systems, such as active cruise control,
      or lane keeping assist. These are
      typically implemented as a set of disparate algorithm-sensor components (
      active cruise control requires a radar, while lane keeping needs a video
      camera). This leads to the <strong>high cost of the sensor suite</strong>.
      More importantly,
      relying on narrow-purpose sensors makes it very <strong>complex
      to extend functionality</strong> in ways that require higher level
      understanding of the environment.
      </li>
  <li>Fully autonomous (SAE level 5) solutions. We believe that in the
      near future, fully autonomous solutions only have potential to succeed in
      well-developed parts of the "first world". Automating the "last 1%" of human
      decision making is 
      <a target="_blank" href="https://spectrum.ieee.org/cars-that-think/transportation/self-driving/have-selfdriving-cars-stopped-getting-better">notoriously difficult</a> by itself, but
      <strong>an order of magnitude more so in the developing world</strong>. There, road
      infrastructure is often poor, and the traffic often follows social rules
      that directly contradict the formal traffic law.</li>
  </ul>
  </p>
  <p>
  Our approach is to <strong>build around computer vision</strong> for sensing.
  With <strong>modern machine learning techniques</strong> 
  (mostly deep learning), it is possible to extract
  dramatically more high-level information from video compared to legacy
  deployed approaches. This immediately leads to multiple benefits:
  <ul>
    <li><strong>Robustness improvements</strong> to traditional ADAS
    functionality. See our demo of simulated lane keep assistance system
    handling a winter road with very little road markings visible.
    </li>
    <li>
    Vehicle <strong>sensor suite can be simplified</strong>. Blind spot
    monitoring reusing panoramic video instead of separate sonars is but one
    example.
    </li>
    <li>
    Most importantly, having high-quality vision data makes it much easier to
    <strong>extend the assistance systems functionality</strong> purely in
    software, gradually <strong>increasing the autonomy level</strong>.
    </li>
  </ul>
  </p>
